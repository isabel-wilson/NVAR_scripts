{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2005727",
   "metadata": {},
   "source": [
    "### Background info\n",
    "\n",
    "Followed the 3-year reliability paper: https://doi.org/10.1016/j.neuroimage.2021.118516\n",
    "1. \"Using these source estimates, we then estimated the power of cortical activity in the canonical frequency bands: delta (2–4 Hz), theta (4–8 Hz), alpha (8–12 Hz), beta (15–30 Hz), low gamma (30–80 Hz), and high gamma (80–150 Hz). We used Welch's method for estimating power spectrum densities (PSD) per four-second epoch across each MEG recording, with 1-second sliding Hamming windows overlapping at 50%. We then standardized the PSD values at each frequency bin to the total power across the frequency spectrum. We then averaged PSD maps (ie. source estimates) across epochs for each participant to obtain one set of six PSD maps (one per frequency band) per participant per visit.\"\n",
    "2. \"We projected these maps onto the MNI ICBM152 brain template (Fonov et al., 2009) and applied a 3 mm FWHM smoothing kernel.\"\n",
    "3. \"Used single rater two-way mixed-effects model and absolute agreement definition, or ICC(A,1)... ICC estimates and their 95% confidence intervals were calculated using the Matlab Central file-exchange ICC.m function. This ICC calculation was applied at every vertex in the PSD maps to obtain spatially specific reliability estimates at each of the frequency bands. This resulted in an ICC map per frequency band.\"\n",
    "4. \"To further visualize the reliability of source power in each frequency band, regions of interest (Brainstorm “scouts”) in the frontal, parietal, temporal, and occipital lobes were applied to each participant's PSD map. The average power (relative to total spectral power) across each lobe was extracted for each participant and each visit. ICCs of these values were then calculated using the same ICC(A,1) model.\"\n",
    "- \"ICC .5 indicates poor reliability, values between .5 to .75 indicates moderate reliability, values between .75 and .9 indicates good reliability, and values greater than .9 indicate excellent reliability. Importantly, we evaluated the level of reliability based on the 95% confidence interval of the ICC estimate, not the estimate itself, since the interval reveals the chance that the true ICC value lands on any point between the bounds.\"\n",
    "\n",
    "### Morph\n",
    "\n",
    "Morph before sprint\n",
    "- The 3yr reliability paper applied a 3mm smoothing kernel\n",
    "Sources: \n",
    "- https://mne.tools/stable/auto_examples/inverse/morph_surface_stc.html\n",
    "\n",
    "### ICC\n",
    "\n",
    "https://github.com/raphaelvallat/pingouin/blob/dcfdc82bbc7f1ba5991b80717a5ca156617443e8/pingouin/reliability.py#L158\n",
    "ICC2 (single random raters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8656a6e",
   "metadata": {},
   "source": [
    "### Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8cd901c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-loading files:\n",
      "Done loading files.\n"
     ]
    }
   ],
   "source": [
    "# Packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pingouin as pg\n",
    "import os\n",
    "import pickle\n",
    "from functools import partial\n",
    "from multiprocessing import Pool\n",
    "\n",
    "# Directory containing _fooof csv files\n",
    "base_dir = \"/home/isw3/scratch/sprint/output_noSSP_nomorph_winL4\"\n",
    "\n",
    "# Constants\n",
    "N_WINDOWS = 115 # 0 to 114\n",
    "N_VERTICES = 8196\n",
    "N_ITERATIONS = 100\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# WITHIN-SCAN\n",
    "\n",
    "# Filenames: Day 1, rest 1\n",
    "runnames = [\"sub_NVAR008_251016_rest1\", \n",
    "            \"sub_NVAR010_251027_rest1\"]\n",
    "\n",
    "r = 2\n",
    "\n",
    "# Pre-generate list of random windows\n",
    "windowAs = []\n",
    "for i in range(N_ITERATIONS):\n",
    "    np.random.seed(i)\n",
    "    windowA = np.random.choice(range(N_WINDOWS - r))\n",
    "    windowAs.append(windowA)\n",
    "\n",
    "\n",
    "# Pre-load all exponents\n",
    "# data_cache is a dictionary; format data_cache[runname][vertex] = series of exponents\n",
    "print(\"Pre-loading files:\")\n",
    "data_cache = {}\n",
    "for runname in runnames:\n",
    "    data_cache[runname] = {}\n",
    "    for vertex in range(N_VERTICES):\n",
    "        file_path = os.path.join(base_dir, runname + \"_fooof_vertex\" + str(vertex) + \".csv\")\n",
    "        df_file = pd.read_csv(file_path, usecols=[\"exponent\"]) \n",
    "        data_cache[runname][vertex] = df_file[\"exponent\"].values \n",
    "print(\"Done loading files.\")\n",
    "\n",
    "\n",
    "def compute_icc_for_vertex(vertex, windowA): \n",
    "    \"\"\" \n",
    "    Computes ICC on a single vertex\n",
    "    Parameters: idx of one vertex; idx of one window pair\n",
    "    Returns: [vertex idx, icc value] -> will be one row \n",
    "    \"\"\"\n",
    "    print(f\"Now working on vertex {vertex} windowA {windowA}\")\n",
    "\n",
    "    try: \n",
    "        # Create dataframe of ratings\n",
    "        ratings = []\n",
    "        for runname in runnames: # Here, levels are runnames because there is only one per subject, but caution that this won't be true later\n",
    "            ratings.extend([\n",
    "                (runname, \"valA\", data_cache[runname][vertex][windowA]),\n",
    "                (runname, \"valB\", data_cache[runname][vertex][windowA + r])\n",
    "            ])\n",
    "        df_vertex = pd.DataFrame(ratings, columns=[\"TARGET\", \"RATER\", \"RATING\"])\n",
    "\n",
    "        # Compute ICC\n",
    "        try: \n",
    "            results = pg.intraclass_corr(data=df_vertex, targets='TARGET', raters='RATER', ratings='RATING')\n",
    "        except Exception as e:\n",
    "            print(f\"pg.intraclass_corr for {vertex} {windowA} returned error: {e}\")\n",
    "        icc = results.loc[results['Type'] == 'ICC2', 'ICC'].values[0]\n",
    "        icc = windowA\n",
    "\n",
    "        # Return vertex and corresponding ICC\n",
    "        return [vertex, icc]\n",
    "    \n",
    "    except Exception as e: \n",
    "        print(f\"compute_icc_for_vertex for {vertex} {windowA} returned error: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def compute_icc():\n",
    "    rows = []\n",
    "    with Pool() as pool:\n",
    "        for windowA in windowAs: \n",
    "            for result in pool.starmap(compute_icc_for_vertex, [(vertex, windowA) for vertex in range(N_VERTICES)]):\n",
    "                if result is not None:\n",
    "                    rows.append([windowA] + result)\n",
    "\n",
    "    df = pd.DataFrame(rows, columns=[\"WINDOW_A\", \"VERTEX\", \"ICC\"])\n",
    "    \n",
    "    # Fisher transform; won't work for values of exactly -1 or 1 so clip first\n",
    "    df[\"ICC_Z\"] = np.arctanh(df[\"ICC\"].clip(-0.9999, 0.9999))\n",
    "\n",
    "    with open(os.path.join(base_dir, \"ICC_\" + str(r) + \".pkl\"), \"wb\") as f:\n",
    "        pickle.dump(df, f)\n",
    "\n",
    "    # Average per vertex\n",
    "    df_avg = df.groupby(\"VERTEX\")[\"ICC_Z\"].mean().reset_index()\n",
    "    with open(os.path.join(base_dir, \"ICC_perVertex_\" + str(r) + \".pkl\"), \"wb\") as f:\n",
    "        pickle.dump(df_avg, f)\n",
    "\n",
    "\n",
    "\n",
    "compute_icc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e6c397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-loading files:\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'runnames' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 29\u001b[39m\n\u001b[32m     27\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mPre-loading files:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     28\u001b[39m data_cache = {}\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m runname \u001b[38;5;129;01min\u001b[39;00m \u001b[43mrunnames\u001b[49m:\n\u001b[32m     30\u001b[39m     data_cache[runname] = {}\n\u001b[32m     31\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m vertex \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(N_VERTICES):\n",
      "\u001b[31mNameError\u001b[39m: name 'runnames' is not defined"
     ]
    }
   ],
   "source": [
    "# FOR LATER\n",
    "# BETWEEN-SCANS: \n",
    "\n",
    "# Runames: To start, always use visit_1_scan_1 as the reference\n",
    "comparison_dictionary = {\n",
    "    \"within-run\" : {\n",
    "        \"sub_NVAR008\" : [\"sub_NVAR008_251016_rest1\", \"sub_NVAR008_251016_rest1\"], \n",
    "        \"sub_NVAR010\" : [\"sub_NVAR010_251027_rest1\", \"sub_NVAR010_251027_rest1\"], \n",
    "        \"sub_NVAR999\" : [\"sub_NVAR999_251016_rest1\", \"sub_NVAR999_251016_rest1\"]\n",
    "    },\n",
    "    \"day\" : {\n",
    "        \"sub_NVAR008\" : [\"sub_NVAR008_251016_rest1\", \"sub_NVAR008_251017_rest1\"], \n",
    "        \"sub_NVAR010\" : [\"sub_NVAR010_251027_rest1\", \"sub_NVAR010_251028_rest1\"], \n",
    "        \"sub_NVAR999\" : [\"sub_NVAR999_251016_rest1\", \"sub_NVAR999_251017_rest1\"]\n",
    "    },\n",
    "    \"week\" : {\n",
    "        \"sub_NVAR008\" : [\"sub_NVAR008_251016_rest1\", \"sub_NVAR008_251023_rest1\"], \n",
    "        \"sub_NVAR010\" : [\"sub_NVAR010_251027_rest1\", \"sub_NVAR010_251103_rest1\"], \n",
    "        \"sub_NVAR999\" : [\"sub_NVAR999_251016_rest1\", \"sub_NVAR999_251023_rest1\"]\n",
    "    },\n",
    "    \"month\" : {\n",
    "        \"sub_NVAR008\" : [\"sub_NVAR008_251016_rest1\", \"sub_NVAR008_251113_rest1\"], \n",
    "        \"sub_NVAR010\" : [\"sub_NVAR010_251027_rest1\", \"sub_NVAR010_251124_rest1\"], \n",
    "        \"sub_NVAR999\" : [\"sub_NVAR999_251016_rest1\", \"sub_NVAR999_251113_rest1\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "print(\"Pre-loading files:\")\n",
    "data_cache = {}\n",
    "scale = comparison_dictionary[\"within-run\"]\n",
    "for subject in scale.keys(): \n",
    "    runnameA = scale[subject][0]\n",
    "    runnameB = scale[subject][1]\n",
    "\n",
    "\n",
    "for runname in runnames:\n",
    "    data_cache[runname] = {}\n",
    "    for vertex in range(N_VERTICES):\n",
    "        file_path = os.path.join(base_dir, runname + \"_fooof_vertex\" + str(vertex) + \".csv\")\n",
    "        df_file = pd.read_csv(file_path, usecols=[\"exponent\"]) \n",
    "        data_cache[runname][vertex] = df_file[\"exponent\"].values \n",
    "print(\"Done loading files.\")\n",
    "\n",
    "\n",
    "\n",
    "ratings = []\n",
    "scale = comparison_dictionary[\"within-run\"]\n",
    "for subject in scale.keys(): \n",
    "    runnameA = scale[subject][0]\n",
    "    runnameB = scale[subject][1]\n",
    "\n",
    "    ratings.extend([\n",
    "            (subject, \"valA\", data_cache[runnameA][vertex][windowA]),\n",
    "            (subject, \"valB\", data_cache[runnameB][vertex][windowA + r])\n",
    "        ])\n",
    "    df_vertex = pd.DataFrame(ratings, columns=[\"TARGET\", \"RATER\", \"RATING\"])\n",
    "\n",
    "\n",
    "\n",
    " #   \"40min\" : {\n",
    " #       \"sub_NVAR008\" : [\"sub_NVAR008_251016_rest1\", \"sub_NVAR008_251016_rest2\"], \n",
    " #       \"sub_NVAR010\" : [\"sub_NVAR010_251027_rest1\", \"sub_NVAR010_251027_rest2\"], \n",
    " #       \"sub_NVAR999\" : [\"sub_NVAR999_251016_rest1\", \"sub_NVAR999_251016_rest2\"]\n",
    " #   },"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cbb61b89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 2, 0], [1, 3, 1], [2, 4, 2], [3, 5, 3]]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows = []\n",
    "\n",
    "for i in [0, 1, 2, 3]: \n",
    "    rows.append([i, i+2] + [i])\n",
    "\n",
    "rows"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
